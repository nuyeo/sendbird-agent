import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

load_dotenv()

# ì „ì—­ ë³€ìˆ˜
qa_chain = None


def initialize_rag():
    """
    ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰: data/faq.txtë¥¼ ì½ì–´ í•™ìŠµ
    """
    global qa_chain

    # ğŸ“Œ [ìˆ˜ì • í¬ì¸íŠ¸ 1] ê²½ë¡œ ì„¤ì • ë¡œì§ ì¶”ê°€
    # í˜„ì¬ íŒŒì¼(app/rag.py)ì˜ ë¶€ëª¨(app)ì˜ ë¶€ëª¨(root)ë¥¼ ì°¾ìŒ
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # data/faq.txt ê²½ë¡œ ì™„ì„±
    file_path = os.path.join(BASE_DIR, "data", "faq.txt")

    # ë²¡í„° DB ì €ì¥ ê²½ë¡œ (data/chroma_db í´ë”ì— ì €ì¥)
    db_path = os.path.join(BASE_DIR, "data", "chroma_db")

    if os.path.exists(db_path) and os.listdir(db_path):
        print("ğŸ’¾ [AI Init] ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤... (ë¹„ìš© ì ˆì•½)")
        embeddings = OpenAIEmbeddings()
        db = Chroma(persist_directory=db_path, embedding_function=embeddings)
    else:
        print(f"ğŸ¤– [AI Init] ë¬¸ì„œë¥¼ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤... (API í˜¸ì¶œ)")
        print(f"ğŸ¤– [AI Init] ë‹¤ìŒ ë¬¸ì„œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤: {file_path}")

        # 1. ë¬¸ì„œ ë¡œë“œ (ê²½ë¡œ ìˆ˜ì •ë¨)
        if not os.path.exists(file_path):
            print(f"ğŸš¨ [Error] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        # 2. ë¬¸ì„œ ìª¼ê°œê¸°
        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
        texts = text_splitter.split_documents(documents)

        # 3. ì„ë² ë”© & ë²¡í„° DB ìƒì„± (ê²½ë¡œ ìˆ˜ì •ë¨)
        embeddings = OpenAIEmbeddings()

        # persist_directoryë¥¼ ì§€ì •í•˜ë©´ DB íŒŒì¼ì´ data/chroma_dbì— ì˜ˆì˜ê²Œ ì €ì¥ë©ë‹ˆë‹¤.
        db = Chroma.from_documents(texts, embeddings, persist_directory=db_path)

    # 4. ê²€ìƒ‰ê¸° ìƒì„±
    retriever = db.as_retriever(search_kwargs={"k": 2})

    # 5. LLM ì„¤ì •
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

    # 6. Chain ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever
    )
    print("âœ… [AI Init] í•™ìŠµ ì™„ë£Œ! AIê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")


def get_ai_response(user_query: str) -> str:
    if qa_chain is None:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    response = qa_chain.invoke(user_query)
    return response['result']